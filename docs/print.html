<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>AirlandKK&#x27;学习笔记</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="algorithm/about.html"><strong aria-hidden="true">1.</strong> 数据结构与算法</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="algorithm/inear_structure/algorithmAbout.html"><strong aria-hidden="true">1.1.</strong> 线性结构</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="algorithm/inear_structure/list.html"><strong aria-hidden="true">1.1.1.</strong> 线性结构</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="bigData/about.html"><strong aria-hidden="true">2.</strong> 大数据</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="bigData/linux/aboutlinux.html"><strong aria-hidden="true">2.1.</strong> Linux</a></li><li class="chapter-item expanded "><a href="bigData/Hadoop/hadoopabout.html"><strong aria-hidden="true">2.2.</strong> Hadoop</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="bigData/Hadoop/typeandbuild.html"><strong aria-hidden="true">2.2.1.</strong> 三种模式及搭建环境</a></li><li class="chapter-item expanded "><a href="bigData/Hadoop/HDFS.html"><strong aria-hidden="true">2.2.2.</strong> HDFS</a></li><li class="chapter-item expanded "><a href="bigData/Hadoop/MapReduce.html"><strong aria-hidden="true">2.2.3.</strong> MapReduce</a></li><li class="chapter-item expanded "><a href="bigData/Hadoop/question.html"><strong aria-hidden="true">2.2.4.</strong> 问题</a></li><li class="chapter-item expanded "><a href="bigData/Hadoop/facetest.html"><strong aria-hidden="true">2.2.5.</strong> 面试</a></li></ol></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">AirlandKK&#x27;学习笔记</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="算法"><a class="header" href="#算法">算法</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="线性结构"><a class="header" href="#线性结构">线性结构</a></h1>
<p>线性结构是一个有序数据元素的集合。 常用的线性结构有：线性表，栈，队列，双队列，串(一维数组)。
关于广义表、数组(高维)，是一种非线性的数据结构。 常见的非线性结构有：二维数组，多维数组，广义表，树(二叉树等)，图</p>
<h2 id="特征"><a class="header" href="#特征">特征</a></h2>
<ol>
<li>集合中必存在唯一的一个&quot;第一个元素&quot;；</li>
<li>集合中必存在唯一的一个&quot;最后的元素&quot;；</li>
<li>除最后元素之外，其它数据元素均有唯一的&quot;后继&quot;；</li>
<li>除第一元素之外，其它数据元素均有唯一的&quot;前驱&quot;。
数据结构中线性结构指的是数据元素之间存在着“一对一”的线性关系的数据结构。<br />
如（a0,a1,a2,.....,an）,a0为第一个元素，an为最后一个元素，此集合即为一个线性结构的集合。
相对应于线性结构，非线性结构的逻辑特征是一个结点元素可能对应多个直接前驱和多个后继。</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="线性结构-1"><a class="header" href="#线性结构-1">线性结构</a></h1>
<h2 id="什么是线性表"><a class="header" href="#什么是线性表">什么是线性表？</a></h2>
<p>多项式表示问题的启示：</p>
<ol>
<li>同一个问题可以有不同的表示（存储）方式</li>
<li>有一类共性问题：有序线性序列的组织和管理</li>
</ol>
<p>“线性表”：由同类型数据元素构成有序序列的线性结构</p>
<ul>
<li>表中元素个数称为线性表的<strong>长度</strong></li>
<li>线性表没有元素时，称为<strong>空表</strong></li>
<li>表起始位置称为<strong>表头</strong>，表结束位置称<strong>表尾</strong></li>
</ul>
<p><img src="algorithm/inear_structure/img/list1.png" alt="线性表的增删改查" /></p>
<h3 id="线性表的链式存储实现"><a class="header" href="#线性表的链式存储实现">线性表的链式存储实现</a></h3>
<ul>
<li>不要求逻辑上相邻的两个元素物理上也相邻；通过“链”建立起数据元素之间的逻辑关系。</li>
<li>插入、删除不需要移动数据元素，只需要修改“链”。</li>
</ul>
<h4 id="广义表"><a class="header" href="#广义表">广义表</a></h4>
<p>我们知道了一元多项式的表示，那么二元多项式又该如何表示？<br />
<img src="algorithm/inear_structure/img/list1.png" alt="" /></p>
<ul>
<li>广义表是线性表的推广</li>
<li>对于线性表而言，n个元素都是基本的单元素；</li>
<li>广义表中，这些元素不仅可以是单元素也可以是另一个广义表。</li>
</ul>
<h4 id="多重链表"><a class="header" href="#多重链表">多重链表</a></h4>
<p>链表中的节点可能同时隶属于多个链</p>
<ul>
<li>多重链表中结点的<strong>指针域会有多个</strong>，如前面例子包含了Next和SubList两个指针域；</li>
<li>但包含两个指针域的链表并不一定是多重链表，比如在<strong>双向链表不是多重链表。</strong></li>
<li>多重链表有广泛的用途：基本上如树、图这样相对复杂的数据结构都可以采用多重链表方式实现存储。<br />
<img src="algorithm/inear_structure/img/list3.png" alt="" /><br />
<img src="algorithm/inear_structure/img/list4.png" alt="" /> 这就是稀疏矩阵用十字链表解决的思路<br />
<img src="algorithm/inear_structure/img/list5.png" alt="" /></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="大数据"><a class="header" href="#大数据">大数据</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="linux"><a class="header" href="#linux">linux</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hadoop1x和hadoop2x区别"><a class="header" href="#hadoop1x和hadoop2x区别">Hadoop1.x和Hadoop2.x区别</a></h1>
<ul>
<li>Hadoop1.x组成
<ul>
<li>MapReduce（计算+资源调度）</li>
<li>HDFS（数据存储）</li>
<li>Common（辅助工具）</li>
</ul>
</li>
<li>Hdoop2.x组成
<ul>
<li>MapReduce（计算）</li>
<li><strong>Yarn（资源调度）</strong></li>
<li>HDFS（数据存储）</li>
<li>Common（辅助工具）</li>
</ul>
</li>
</ul>
<p>在Hadoop1.X时代，Hadoop中的MapRudece同时处理业务逻辑运算和资源的调度，耦合性较大，在Hadoop2.X时代，增加了Yarn。Yarn只负责资源的调度，MapReduce只负责运算。</p>
<h1 id="hdfs架构概述"><a class="header" href="#hdfs架构概述">HDFS架构概述</a></h1>
<ul>
<li><strong>NameNode（nn）</strong>-类似目录：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。</li>
<li><strong>DataNode（dn）</strong>-类似真正存储的数据：在本地文件系统存储文件块数据，以及块数据的校验和。</li>
<li>Secondary NameNode（2nn）：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。</li>
</ul>
<h1 id="yarn架构概述"><a class="header" href="#yarn架构概述">YARN架构概述</a></h1>
<ol>
<li><strong>ResourceManager（RM）主要作用如下</strong>
<ol>
<li>处理客户端请求</li>
<li>监控NodeManager</li>
<li>启动或监控ApplicationMaster</li>
<li>资源的分配与调度</li>
</ol>
</li>
<li><strong>NodeManager（NM）主要作用如下</strong>
<ol>
<li>管理单个节点上的资源</li>
<li>处理来自ResourceManager的命令</li>
<li>处理来自ApplicationMaster的命令</li>
</ol>
</li>
<li>ApplicationMaster（AM）作用如下
<ol>
<li>负责数据的切分</li>
<li>为应用程序申请资源并分配给内部的任务</li>
<li>任务的监控与容错</li>
</ol>
</li>
<li>Container
<ol>
<li>Container是YARN中资源抽象，它封住了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。</li>
</ol>
</li>
</ol>
<h1 id="maprudece架构概述"><a class="header" href="#maprudece架构概述">MapRudece架构概述</a></h1>
<p>MapReduce将计算过程分为两个阶段：Map和Reduce</p>
<ol>
<li>Map阶段并行处理输入数据</li>
<li>Reduce阶段对Map结果进行汇总</li>
</ol>
<p><img src="bigData/Hadoop/img/image-20220117143520454.png" alt="image-20220117143520454" /></p>
<h1 id="大数据技术生态系统"><a class="header" href="#大数据技术生态系统">大数据技术生态系统</a></h1>
<table><thead><tr><th>数据来源层</th><th>数据传输层</th><th>数据存储层</th><th>资源管理层</th><th>数据计算层</th><th>任务调度层</th><th>配置和调度</th><th>业务模型层</th></tr></thead><tbody>
<tr><td>数据库（结构化数据）</td><td>Sqoop数据传递</td><td><strong>HDFS文件存储</strong></td><td>YARN资源管理</td><td>MapReduce<u>离线</u>计算（1. <strong>Hive数据查询</strong>（javaEE）、2. Mahout数据挖掘（算法））</td><td>Oozie任务调度、Azkaban任务调度</td><td>Zookeeper（容易改变的配置信息）</td><td>业务模型、数据库可视化、业务应用</td></tr>
<tr><td>文件日志（半结构化数据）</td><td>Flume日志收集</td><td><strong>HDFS文件存储</strong>/HBase非关系型数据库</td><td>YARN资源管理</td><td>Spark Core内存计算 （<u>离线</u>1. Spark Mlib数据挖掘、2.Spark R数据分析、3.Spark Sql数据查询；<u>准实时批处理</u>Spark Streaming实时计算）/Flink 流处理</td><td>Oozie任务调度、Azkaban任务调度</td><td>Zookeeper（容易改变的配置信息）</td><td>业务模型、数据库可视化、业务应用</td></tr>
<tr><td>视频、ppt等（非结构化数据）</td><td>Kafka消息队列</td><td>Kafka缓存一些数据</td><td></td><td></td><td></td><td>Zookeeper（容易改变的配置信息）</td><td>业务模型、数据库可视化、业务应用</td></tr>
</tbody></table>
<p><img src="bigData/Hadoop/img/image-20211226125619030.png" alt="image-大数据技术生态系统" /></p>
<h1 id="推荐系统项目框架"><a class="header" href="#推荐系统项目框架">推荐系统项目框架</a></h1>
<p><img src="bigData/Hadoop/img/image-20211226132415451.png" alt="image-推荐系统" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="单机模式-stonealone"><a class="header" href="#单机模式-stonealone">单机模式 stonealone</a></h1>
<ul>
<li>grep案例</li>
<li>WordCount案例</li>
</ul>
<h1 id="伪分布式模式"><a class="header" href="#伪分布式模式">伪分布式模式</a></h1>
<ul>
<li>所有配置都是按照分布式来的</li>
<li>但是只有一台服务器</li>
</ul>
<h1 id="虚拟机环境准备"><a class="header" href="#虚拟机环境准备">虚拟机环境准备</a></h1>
<pre><code class="language-shell">vim /etc/udev/rules.d/70-persistent-net.rules //修改最后为eth0
vim /etc/sysconfig/network-scripts/ifcfg-eth0 

</code></pre>
<p>https://juejin.cn/post/6991352348471722014#heading-18</p>
<p>https://juejin.cn/post/6844904114980126734#heading-1</p>
<pre><code class="language-shell"># 设置环境变量
$ vim /etc/profile
# 进入之后在文件末尾追加如下内容：
#java
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk
export JRE_HOME=${JAVA_HOME}/jre    
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib    
export PATH=${JAVA_HOME}/bin:$PATH
#hadoop
export HADOOP_HOME=/opt/software/hadoop-3.3.1
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;
export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH
# 使环境变量生效
$ source /etc/profile

	&lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/opt/software/hadoop-3.3.1/data/tmp&lt;/value&gt;
    &lt;/property&gt;
</code></pre>
<h1 id="配置免密登录"><a class="header" href="#配置免密登录">配置免密登录</a></h1>
<h4 id="1编辑etchosts"><a class="header" href="#1编辑etchosts">1.编辑/etc/hosts</a></h4>
<p>(下面的 IPn 表示如 192.168.1.1 格式的云服务器外网 IP 地址。注意，如果是指向本机的 IP，请用内网 IP 地址代替)</p>
<pre><code class="language-XML">IP1 ZKK01
IP2 slave1
IP3 slave2
</code></pre>
<h4 id="2切换到hadoop用户生成id_rsapub我是root用户所以不用切换和赋权"><a class="header" href="#2切换到hadoop用户生成id_rsapub我是root用户所以不用切换和赋权">2.切换到hadoop用户生成id_rsa.pub(我是root用户所以不用切换和赋权)</a></h4>
<pre><code class="language-shell">su hadoop
cd ~
ssh-keygen -t rsa
cd ~/.ssh/
cat id_rsa.pub &gt;&gt; authorized_keys
#chmod 700 /home/hadoop/.ssh
#chmod 644 /home/hadoop/.ssh/authorized_keys
</code></pre>
<p>---<strong>以上命令所有云服务器都要运行</strong>---</p>
<h4 id="3交换共享-id_rsapub-的内容"><a class="header" href="#3交换共享-id_rsapub-的内容">3.交换共享 id_rsa.pub 的内容</a></h4>
<p>（如果搭建伪分布模式，则可以略过交换共享这一步，直接进行 ssh 的测试）--我就是伪分布式</p>
<h5 id="1-master-云服务器操作"><a class="header" href="#1-master-云服务器操作">1) master 云服务器操作</a></h5>
<pre><code>scp /home/hadoop/.ssh/authorized_keys slave2:/home/hadoop/.ssh/
</code></pre>
<h5 id="2-slave1-云服务器操作"><a class="header" href="#2-slave1-云服务器操作">(2) slave1 云服务器操作</a></h5>
<pre><code>scp /home/hadoop/.ssh/authorized_keys slave3:/home/hadoop/.ssh/
</code></pre>
<h5 id="3-slave2-云服务器操作"><a class="header" href="#3-slave2-云服务器操作">(3) slave2 云服务器操作</a></h5>
<pre><code>scp /home/hadoop/.ssh/authorized_keys master:/home/hadoop/.ssh/
</code></pre>
<ul>
<li>这一步的最终目的是让所有云服务器的 <strong>authorized_keys</strong> 内容都包含各自的 <strong>id_rsa.pub</strong> 信息，且内容相同。</li>
</ul>
<h5 id="4-测试配置是否成功"><a class="header" href="#4-测试配置是否成功">(4) 测试配置是否成功</a></h5>
<h6 id="master-上执行命令"><a class="header" href="#master-上执行命令">master 上执行命令：</a></h6>
<pre><code>ssh slave1
quit
ssh slave2
quit
</code></pre>
<h6 id="slave1-上执行命令"><a class="header" href="#slave1-上执行命令">slave1 上执行命令：</a></h6>
<pre><code>ssh master
quit
ssh slave2
quit
</code></pre>
<h6 id="slave2-上执行命令"><a class="header" href="#slave2-上执行命令">slave2 上执行命令：</a></h6>
<pre><code>ssh master
quit
ssh slave1
quit
</code></pre>
<ul>
<li>需要确保所有云服务器能够相互 <code>ssh</code> 通过。</li>
<li>第一次进行 <code>ssh</code> 需要密码登录。输完密码之后，选择 <code>yes</code> 保存记录。之后就不再需要输入密码登录了。</li>
<li>如果出现异常情况，可重启服务再尝试：<code>sudo service sshd service</code>。</li>
</ul>
<h1 id="修改配置文件"><a class="header" href="#修改配置文件">修改配置文件</a></h1>
<h4 id="1-etcprofile-配置环境变量"><a class="header" href="#1-etcprofile-配置环境变量">1. /etc/profile 配置环境变量</a></h4>
<pre><code class="language-shell">vim /opt/software/hadoop-3.3.1/etc/profile
</code></pre>
<p><img src="bigData/Hadoop/img/image-20211228212644519.png" alt="环境变量" /></p>
<pre><code class="language-XML">#java
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH
#hadoop
export HADOOP_HOME=/opt/software/hadoop-3.3.1
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;
export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH
</code></pre>
<h4 id="2-使环境变量生效"><a class="header" href="#2-使环境变量生效">2. 使环境变量生效</a></h4>
<pre><code class="language-she'l">source /etc/profile
</code></pre>
<h4 id="3进入这三个文件-每个文件都添加"><a class="header" href="#3进入这三个文件-每个文件都添加">3.进入这三个文件 每个文件都添加<img src="bigData/Hadoop/img/image-20220106211454302.png" alt="image-20220106211454302" /></a></h4>
<pre><code class="language-shell">vim hadoop-env.sh
 export JAVA_HOME=/root/apps/jdk(JDK安装目录)
</code></pre>
<h1 id="集群配置"><a class="header" href="#集群配置">集群配置</a></h1>
<p><img src="bigData/Hadoop/img/image-20211228203703927.png" alt="image-20211228203703927" /></p>
<p>也可以都配置在同一台机子上如果内存够的话..</p>
<p>四个默认核心文件</p>
<p>四个自定义文件：</p>
<pre><code class="language-shell">#配置路径
/opt/software/hadoop-3.3.1/etc/hadoop
</code></pre>
<ul>
<li>配置core-site.xml</li>
</ul>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;

&lt;!-- Put site-specific property overrides in this file. --&gt;

&lt;configuration&gt;
    &lt;!--指定HDFS中NameNode的地址--&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://ZKK01:8020&lt;/value&gt;
		&lt;!-- 除了8020 还有9000等 --&gt;
    &lt;/property&gt;
    &lt;!--指定Hadoop运行时产生文件的存储目录--&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/opt/software/hadoop-3.3.1/data/tmp&lt;/value&gt;
    &lt;/property&gt;
    &lt;!--配置HDFS网页登录使用的静态用户为zkk，可以不配置--&gt;
     &lt;property&gt;
        &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;
        &lt;value&gt;ZKK01&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;

</code></pre>
<ul>
<li>配置hdfs-site.xml</li>
</ul>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;

&lt;configuration&gt;
    &lt;!-- nn web端访问地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;
        &lt;value&gt;ZKK01:9870&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 指定HDFS副本的数量 --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 2nn web端访问地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
        &lt;value&gt;ZKK01:9870&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<ul>
<li>配置yarn-site.xml</li>
</ul>
<pre><code class="language-xml">&lt;configuration&gt;
    &lt;!-- 指定MR走shuffle--&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
		&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
	&lt;/property&gt;
    
    &lt;!--指定ResourceManager地址--&gt;
    &lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
		&lt;value&gt;ZKK01&lt;/value&gt;
	&lt;/property&gt;

    &lt;!--环境变量的继承--&gt;
    &lt;!--3.1.3以上的版本解决了这个bug可以不配置--&gt;
&lt;/configuration&gt;
</code></pre>
<ul>
<li>配置mapred-site.xml</li>
</ul>
<pre><code class="language-xml">&lt;configuration&gt;
    &lt;!--指定MapReduce程序运行在Yarn上--&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
		&lt;value&gt;yarn&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<h1 id="群起集群"><a class="header" href="#群起集群">群起集群</a></h1>
<h2 id="1-配置works"><a class="header" href="#1-配置works">1. 配置Works</a></h2>
<pre><code class="language-shell">vim /opt/software/hadoop-3.3.1/etc/hadoop/workers
</code></pre>
<pre><code class="language-xml">#默认为local 把local注释掉
ZKK01
</code></pre>
<h2 id="2-启动集群"><a class="header" href="#2-启动集群">2. 启动集群</a></h2>
<ol>
<li><strong>如果集群是第一次启动</strong>，需要在ZKK01节点格式化NameNode（注意：格式化NameNode，会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到以往数据。如果集群在运行中报错，需要重新格式化NameNode的话，一定要先停止namenode和datanode进程，并且要删除所有机器的data和logs目录，然后再进行格式化）</li>
</ol>
<pre><code class="language-shell">[root@ZKK01 hadoop-3.3.1]# hdfs namenode -format
</code></pre>
<ol start="2">
<li>启动HDFS</li>
</ol>
<pre><code class="language-shell">[root@ZKK01 hadoop-3.3.1]# sbin/start-dfs.sh
</code></pre>
<p>/opt/software/hadoop-3.3.1/data/tmp/dfs/name/current</p>
<p><img src="bigData/Hadoop/img/image-20211228210452263.png" alt="image-20211228210452263" /></p>
<ul>
<li>启动过程中遇到报错（<strong>root权限问题?</strong>）</li>
</ul>
<p><img src="bigData/Hadoop/img/image-20211228210901959.png" alt="image-20211228210901959" /></p>
<p>解决办法：</p>
<ul>
<li>
<p>方法一：</p>
<p>在Hadoop安装目录下找到sbin文件夹</p>
<p>在里面修改四个文件</p>
<p>1、对于start-dfs.sh和stop-dfs.sh文件，添加下列参数：</p>
<pre><code class="language-xml">#!/usr/bin/env bash
HDFS_DATANODE_USER=root
HADOOP_SECURE_DN_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
</code></pre>
<p>2、对于start-yarn.sh和stop-yarn.sh文件，添加下列参数：</p>
<pre><code class="language-shell">#!/usr/bin/env bash
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root

</code></pre>
<p>重新开始start…就可以。</p>
</li>
<li>
<p>方法二（推荐采用）</p>
<p><img src="bigData/Hadoop/img/image-20211228211221848.png" alt="错误解决" /></p>
<ul>
<li>
<pre><code class="language-shell">cd /etc/hadoop/
vim hadoop-env.sh
</code></pre>
</li>
<li>
<pre><code class="language-xml">export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root

</code></pre>
</li>
<li>
<p>Now save and start yarn, hdfs service and check that it works.</p>
<p>我们在hadoop-env.sh文件中也可以找到如下的描述</p>
<p>To prevent accidents, shell commands be (superficially) locked to only allow certain users to execute certain subcommands.</p>
<p>为了防止发生意外，仅（部分）锁定shell命令以仅允许某些用户执行某些子命令。</p>
<p>It uses the format of (command)_(subcommand)_USER.For example, to limit who can execute the namenode command,export HDFS_NAMENODE_USER=hdfs</p>
<p>使用“命令_子命令_用户”，例如，通过使用export HDFS_NAMENODE_USER=hdfs来限制哪个用户可以执行namenode命令。参考stackoverflow上的相关讨论</p>
</li>
</ul>
</li>
</ul>
<ol start="3">
<li>
<p>Web访问NameNode Web地址</p>
<ul>
<li>
<p>IP：9870（配置hdfs-site.xml时候设置的）</p>
</li>
<li>
<p>查看HDFS上存储的数据信息</p>
</li>
<li>
<p>我是用云服务器的 所以记得在云服务器防火墙管理中添加可访问端口（其他端口同理）<img src="bigData/Hadoop/img/image-20211228213933435.png" alt="防火墙" /></p>
</li>
</ul>
</li>
<li>
<p>在配置了<strong>ResourceManager</strong>的节点（ZKK01）启动YARN</p>
</li>
</ol>
<ul>
<li>
<pre><code class="language-shell">[root@ZKK01 hadoop-3.3.1]# sbin/start-yarn.sh 
</code></pre>
</li>
<li>
<p><img src="bigData/Hadoop/img/image-20211228214842406.png" alt="image-20211228214842406" /></p>
</li>
</ul>
<ol start="5">
<li>Web端查看YARN的ResourceManager
<ul>
<li>IP：8088</li>
<li>查看YARN上运行的Job信息</li>
</ul>
</li>
</ol>
<h1 id="集群基本测试"><a class="header" href="#集群基本测试">集群基本测试</a></h1>
<h4 id="1上传文件到集群"><a class="header" href="#1上传文件到集群">1.上传文件到集群</a></h4>
<p>上传小文件</p>
<pre><code class="language-shell">hadoop fs -mkdir /wcinput
hadoop fs -put wcinput/wc.input /wcinput
</code></pre>
<p><img src="bigData/Hadoop/img/image-20220103153532117.png" alt="image-20220103153532117" /></p>
<p>网页中操作，要在本地机配置hosts文件</p>
<h4 id="2查看hdfs在磁盘存储文件内容"><a class="header" href="#2查看hdfs在磁盘存储文件内容">2.查看HDFS在磁盘存储文件内容</a></h4>
<ol>
<li>查看：页面只是一个链接，真实的东西都存在data节点上。</li>
</ol>
<pre><code class="language-shell">cd data/tmp/dfs/data/current/BP-912934988-110.42.160.28-1641041699096/current/finalized/subdir0/subdir0/

cat blk_1073741825
zhangkeke hadoop zhangsan lisi wangwu 
liuliu wangmengting xingguo 
zhangkeke 
keke keke keke keke keke hadoop
hive hivehive

</code></pre>
<ol start="2">
<li>
<p>拼接：默认块大小128MB如上传一个jdk tar包，其实也是放在节点上 可通过拼接命令查看</p>
<p><img src="bigData/Hadoop/img/image-20220103160451159.png" alt="image-20220103160451159" /></p>
</li>
</ol>
<h4 id="3下载"><a class="header" href="#3下载">3.下载</a></h4>
<pre><code class="language-shell">hadoop fs -get /XXX ./
</code></pre>
<h4 id="4执行wordcount程序"><a class="header" href="#4执行wordcount程序">4.执行wordcount程序</a></h4>
<pre><code class="language-sehll">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount /wcinput /wcoutput
</code></pre>
<h1 id="配置历史服务器"><a class="header" href="#配置历史服务器">配置历史服务器</a></h1>
<p><strong>3.2以上的版本没配置也能跳转</strong></p>
<pre><code class="language-xml">    &lt;!--历史服务器端地址--&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
        &lt;value&gt;master:10020&lt;/value&gt;
    &lt;/property&gt;
    &lt;!--历史服务器web端地址--&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
        &lt;value&gt;master:19888&lt;/value&gt;
    &lt;/property&gt;
</code></pre>
<p>需要手动启动历史服务器</p>
<pre><code class="language-shell">mapred --daemon start historyserver
</code></pre>
<h1 id="开启日志聚集功能"><a class="header" href="#开启日志聚集功能">开启日志聚集功能！</a></h1>
<p>伪分布式就不用了</p>
<p><img src="bigData/Hadoop/img/image-20220103174805436.png" alt="image-20220103174805436" /></p>
<h1 id="hadoop集群的群起脚本"><a class="header" href="#hadoop集群的群起脚本">hadoop集群的群起脚本</a></h1>
<p>https://www.codetd.com/article/1452178</p>
<div style="break-before: page; page-break-before: always;"></div><p>[TOC]</p>
<img src="bigData/Hadoop/img/image-20220103202221934.png" alt="HDFS概览" style="zoom:70%;float:left" />
<h1 id="第一章-hdfs概述"><a class="header" href="#第一章-hdfs概述">第一章 HDFS概述</a></h1>
<h2 id="11hdfs产出背景及定义"><a class="header" href="#11hdfs产出背景及定义">1.1HDFS产出背景及定义</a></h2>
<ol>
<li>
<p>HDFS产出背景</p>
<p>随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS 只是分布式文件管理系统中的一种。</p>
</li>
<li>
<p>HDFS定义</p>
<p>HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过<strong>目录树</strong>来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，<strong>集群中的服务器有各自的角色</strong>。</p>
<p>HDFS 的使用场景：<strong>适合一次写入，多次读出的场景</strong>。一个文件经过创建、写入和关闭之后就不需要改变。</p>
</li>
</ol>
<h2 id="12-hdfs优缺点"><a class="header" href="#12-hdfs优缺点">1.2 HDFS优缺点</a></h2>
<p><u>优点</u>：</p>
<ol>
<li>高容错性
<ol>
<li>数据自动保存多个副本。它通过增加副本的形式，提高容错性。</li>
<li>某一个副本丢失以后，它可以自动恢复。</li>
</ol>
</li>
<li>适合处理大数据
<ol>
<li>数据规模：能够处理数据规模达到GB、TB、甚至<strong>PB</strong>级别的数据。</li>
<li>文件规模：能够处理<strong>百万</strong>规模以上的<strong>文件数量</strong>，数量相当之大。</li>
</ol>
</li>
<li><strong>可构建在廉价机器上</strong>，通过多副本机制，提高可靠性</li>
</ol>
<p><u>缺点</u>：</p>
<ol>
<li>低时间延迟的访问</li>
</ol>
<ul>
<li>要求低时间延迟的数据访问的应用，不适合在HDFS上运行，比如毫秒级的存储数据。HDFS是提高数据吞吐量的应用优化的，但可能会以提高时间延迟为代价。</li>
</ul>
<ol start="2">
<li>
<p>无法高效的对大量小文件进行存储：</p>
<ul>
<li>存储大量小文件(这里的小文件是指小于HDFS系统的Block大小的文件（默认64M）)的话，它会占用 NameNode大量的内存来存储文件、目录和块信息。这样是不可取的，因为NameNode的内存总是有限的。</li>
<li>由于namenode将文件系统的元数据存储在内存中，因此文件系统所能存储的文件数量受限制于namenode的内存容量。</li>
<li>小文件存储的寻道时间会超过读取时间，它违反了HDFS的设计目标。</li>
</ul>
<pre><code> NN负责文件元数据(属性，块的映射)的管理，NN在运行时，必须将当前集群中存储所有文件的元数据全部加载到内存！NN耗费大量内存！ 而不能存储可观的数据。
 举例： 当前运行NN的机器，有64G内存，除去系统开销，分配给NN50G内存！
 
 文件a (1k), 存储到HDFS上，需要将a文件的元数据保存到NN，加载到内存
 	包括：文件名  创建时间  所属主  所属组 权限 修改时间+ 块的映射(1块)
 	NN占用内存：150B
 	最多存储50G/150B个文件a
 		存储占用磁盘空间：50G/150B * 1k
 	 
 文件b (128M), 存储到HDFS上，需要将b文件的元数据保存到NN，加载到内存
 		包括：文件名  创建时间  所属主  所属组 权限 修改时间+块的映射(1块)
 	NN占用内存：150B
 	最多存储50G/150B个文件b
 		存储占用磁盘空间：50G/150B * 128M

</code></pre>
</li>
<li>
<p>并发写入，文件随机修改：</p>
<ul>
<li>一个文件只能有一个写，不允许多个线程同时写。</li>
<li><strong>仅支持数据 append（追加）</strong>，不支持文件的随机修改。</li>
</ul>
</li>
</ol>
<h2 id="13hdfs组成架构"><a class="header" href="#13hdfs组成架构">1.3HDFS组成架构</a></h2>
<ol>
<li>
<p><code>NameNode（nn）</code>：就是Master，它是一个主管、管理者</p>
<ol>
<li>管理HDFS的名称空间；</li>
<li>配置副本策略；</li>
<li>管理数据块（Block）映射信息；</li>
<li>处理客户端读写请求。 </li>
</ol>
</li>
<li>
<p><code>DataNode（dn）</code>：就是Slave(3.x之后叫worker)。NameNode下达命令，DataNode执行实际的操作</p>
<ol>
<li>存储实际的数据块；</li>
<li>执行数据块的读/写操作。</li>
</ol>
<img src="bigData/Hadoop/img/image-20220104210422466.png" alt="image-20220104210422466" style="zoom:50%" />
<p>Client：就是客服端</p>
<ol>
<li>文件切分：文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传；</li>
<li>与NameNode交互，获取文件的位置信息；（允不允许读）</li>
<li>与DataNode交互，读取或者写入数据；</li>
<li>Client提供一些命令来管理HDFS，比如NameNode格式化；</li>
<li>Client可以通过一些命令来访问HDFS，比如对HDFS增删改查操作。</li>
</ol>
</li>
<li>
<p>Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。</p>
<ol>
<li>辅助NameNode，分担其工作。</li>
<li>但是我们以后都会用2个NameNode来代替，因为NameNode有高可用的特性。</li>
</ol>
</li>
</ol>
<h2 id="14hdfs-文件块大小面试重点"><a class="header" href="#14hdfs-文件块大小面试重点">1.4HDFS 文件块大小（面试重点）</a></h2>
<p>​	HDFS中的文件再物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）来规定，<strong>默认大小在Hadoop2.x/3.x版本中是128M,1.X版本中是64M。</strong></p>
<ul>
<li>如果寻址时间约为10ms，即查找到目标block的时间为10ms。</li>
<li>寻址时间为传输时间的1%时，则为最佳状态。因此，传输时间=10ms/0.01=1000ms=1s</li>
<li>而目前磁盘的传输速率普遍为100MB/s（固态硬盘200~300M）
<ul>
<li><u>因此：普通硬盘的设置成128M</u></li>
<li><u>固态硬盘块大小可以设置成256M</u></li>
</ul>
</li>
</ul>
<ol>
<li>
<p>为什么块的大小不能设置太小，也不能设置太大？</p>
<ol>
<li>HFDFS的块<strong>设置太小，会增加寻址时间</strong>，程序一直在找块的开始位置；</li>
<li>如果块设置的<strong>太大</strong>，从<strong>磁盘传输数据的时间</strong>会明显** **。导致程序在处理这块数据时，会非常慢。</li>
</ol>
<p>总结：HDFS块的大小设置主要取决于磁盘传输速率。</p>
</li>
</ol>
<h1 id="第二章-hdfs的shell操作开发重点"><a class="header" href="#第二章-hdfs的shell操作开发重点">第二章 HDFS的Shell操作（开发重点）</a></h1>
<h2 id="21-基本语法"><a class="header" href="#21-基本语法">2.1 基本语法</a></h2>
<p>hadoop fs 具体命令 OR hdfs dfs 具体命令</p>
<p>两个完全相同</p>
<h2 id="22-命令大全"><a class="header" href="#22-命令大全">2.2 命令大全</a></h2>
<pre><code class="language-shell">[root@ZKK01 ~]# hadoop fs
2022-01-05 15:11:03,273 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Usage: hadoop fs [generic options]
        [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]
        [-cat [-ignoreCrc] &lt;src&gt; ...]
        [-checksum [-v] &lt;src&gt; ...]
        [-chgrp [-R] GROUP PATH...]
        [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]
        [-chown [-R] [OWNER][:[GROUP]] PATH...]
        [-concat &lt;target path&gt; &lt;src path&gt; &lt;src path&gt; ...]
        [-copyFromLocal [-f] [-p] [-l] [-d] [-t &lt;thread count&gt;] &lt;localsrc&gt; ... &lt;dst&gt;]
        [-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]
        [-count [-q] [-h] [-v] [-t [&lt;storage type&gt;]] [-u] [-x] [-e] [-s] &lt;path&gt; ...]
        [-cp [-f] [-p | -p[topax]] [-d] &lt;src&gt; ... &lt;dst&gt;]
        [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]
        [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]
        [-df [-h] [&lt;path&gt; ...]]
        [-du [-s] [-h] [-v] [-x] &lt;path&gt; ...]
        [-expunge [-immediate] [-fs &lt;path&gt;]]
        [-find &lt;path&gt; ... &lt;expression&gt; ...]
        [-get [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]
        [-getfacl [-R] &lt;path&gt;]
        [-getfattr [-R] {-n name | -d} [-e en] &lt;path&gt;]
        [-getmerge [-nl] [-skip-empty-file] &lt;src&gt; &lt;localdst&gt;]
        [-head &lt;file&gt;]
        [-help [cmd ...]]
        [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [&lt;path&gt; ...]]
        [-mkdir [-p] &lt;path&gt; ...]
        [-moveFromLocal [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]
        [-moveToLocal &lt;src&gt; &lt;localdst&gt;]
        [-mv &lt;src&gt; ... &lt;dst&gt;]
        [-put [-f] [-p] [-l] [-d] [-t &lt;thread count&gt;] &lt;localsrc&gt; ... &lt;dst&gt;]
        [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]
        [-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...]
        [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]
        [-setfacl [-R] [{-b|-k} {-m|-x &lt;acl_spec&gt;} &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]
        [-setfattr {-n name [-v value] | -x name} &lt;path&gt;]
        [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]
        [-stat [format] &lt;path&gt; ...]
        [-tail [-f] [-s &lt;sleep interval&gt;] &lt;file&gt;]
        [-test -[defswrz] &lt;path&gt;]
        [-text [-ignoreCrc] &lt;src&gt; ...]
        [-touch [-a] [-m] [-t TIMESTAMP (yyyyMMdd:HHmmss) ] [-c] &lt;path&gt; ...]
        [-touchz &lt;path&gt; ...]
        [-truncate [-w] &lt;length&gt; &lt;path&gt; ...]
        [-usage [cmd ...]]
</code></pre>
<h2 id="23-常用实操命令"><a class="header" href="#23-常用实操命令">2.3 常用实操命令</a></h2>
<h3 id="231-准备工作"><a class="header" href="#231-准备工作">2.3.1 准备工作</a></h3>
<ol>
<li>启动Hadoop集群（方便后续测试）</li>
</ol>
<pre><code class="language-shell">[root@ZKK01 hadoop-3.3.1]# sbin/start-all.sh
</code></pre>
<ol start="2">
<li>-help：输出这个命令参数的帮助，也可以去百度</li>
</ol>
<pre><code class="language-shell">[root@ZKK01 hadoop-3.3.1]# hadoop fs -help rm
</code></pre>
<ol start="3">
<li>创建/sanguo文件夹</li>
</ol>
<pre><code class="language-shell">[root@ZKK01 hadoop-3.3.1]# hadoop fs -mkdir /sanguo
</code></pre>
<h3 id="232-上传"><a class="header" href="#232-上传">2.3.2 上传</a></h3>
<ol>
<li>
<p>-moveFromLocal 从本地<u>剪切</u>粘贴到HDFS</p>
<pre><code class="language-shell">[root@ZKK01 hadoop-3.3.1]# vim shuguo.txt
输入： shuguo
[root@ZKK01 hadoop-3.3.1]# hadoop fs -moveFromLocal ./shuguo.txt /sanguo
</code></pre>
</li>
<li>
<p>-copyFromLocal 从本地文件系统拷贝文件到HDFS路径去</p>
</li>
<li>
<p>-put 等同于copyFromLocal，生产环境更习惯用put</p>
<pre><code class="language-shell">hadoop fs -put ./wuguo.txt /sanguo
</code></pre>
</li>
<li>
<p>-appendToFile 追加一个文件到已经存在的文件末尾</p>
<pre><code class="language-shell">vim liubei.txt
输入： liubei

hadoop fs -appendToFile liubei.txt /sanguo/shuguo.txt
</code></pre>
</li>
</ol>
<h3 id="233-下载"><a class="header" href="#233-下载">2.3.3 下载</a></h3>
<ol>
<li>
<p>-copyToLocal：从HDFS拷贝到本地</p>
</li>
<li>
<p>-get：等同于copyToLocal，生产环境更习惯用get</p>
<pre><code class="language-shell">hadoop fs -get /sanguo/shuguo.txt ./shuguo2.txt
</code></pre>
</li>
</ol>
<h3 id="234-hdfs-直接操作"><a class="header" href="#234-hdfs-直接操作">2.3.4 HDFS 直接操作</a></h3>
<ol>
<li>
<p>-ls： 显示目录信息</p>
<pre><code class="language-shell">hadoop fs -ls /sanguo
</code></pre>
</li>
<li>
<p>-cat：显示文件内容</p>
<pre><code class="language-shell">hadoop fs -cat /sanguo/shuguo.txt
</code></pre>
</li>
<li>
<p>-chgrp、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</p>
<pre><code class="language-shell">hadoop fs -chmod 666 /sanguo/shuguo.txt
hadoop fs -chmod ZKK01:ZKK01 /sanguo/shuguo.txt
</code></pre>
</li>
<li>
<p>-mkdir：创建路径</p>
<pre><code class="language-shell">hadoop fs -mkdir /jinguo
</code></pre>
</li>
<li>
<p>-cp：从HDFS的一个路径拷贝到HDFS的另一个路径</p>
<pre><code class="language-shell">hadoop fs -cp /sanguo/shuguo.txt /jinguo
</code></pre>
</li>
<li>
<p>-mv：在HDFS目录中移动文件</p>
<pre><code class="language-shell">hadoop fs -mv /sanguo/wuguo.txt /jinguo
hadoop fs -mv /sanguo/weiguo.txt /jinguo
</code></pre>
</li>
<li>
<p>-tail：显示一个文件的末尾1kb的数据</p>
<pre><code class="language-shell">hadoop fs -tail /jinguo/shuguo.txt
</code></pre>
</li>
<li>
<p>-rm：删除文件或文件夹</p>
<pre><code class="language-shell">hadoop -rm /sanguo/shuguo.txt
</code></pre>
</li>
<li>
<p>-rm -r：递归删除目录及目录里面的内容</p>
<pre><code class="language-shell">hadoop fs -rm -r /sanguo
</code></pre>
</li>
<li>
<p>-du：统计文件夹的大小信息</p>
<pre><code class="language-shell">hadoop fs -du -s -h /jinguo #只列出文件夹总大小
hadoop fs -du -h /jinguo	#文件夹内内容详细信息
</code></pre>
</li>
<li>
<p>-setrep：设置HDFS中文件的副本数量</p>
<pre><code class="language-shell">hadoop fs -setrep 10 /jinguo/shuguo.txt
</code></pre>
<p><img src="bigData/Hadoop/img/image-20220105182207891.png" alt="image-20220105182207891" /></p>
<p><img src="bigData/Hadoop/img/image-20220105182227695.png" alt="image-20220105182227695" /></p>
<p>这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。</p>
</li>
</ol>
<h1 id="第三章-hdfs的api操作"><a class="header" href="#第三章-hdfs的api操作">第三章 HDFS的API操作</a></h1>
<h2 id="31客户端环境准备"><a class="header" href="#31客户端环境准备">3.1客户端环境准备</a></h2>
<ol>
<li>
<p>windows去：https://github.com/kontext-tech/winutils 下载对应版本的编译好的bin文件替换原来安装在windows里的hadoop文件的bin</p>
</li>
<li>
<p>将bin文件里的hadoop.dll粘贴到system32里</p>
</li>
<li>
<p>创建maven项目导入相关相关依赖</p>
</li>
</ol>
<h2 id="32hdfs-api相关操作"><a class="header" href="#32hdfs-api相关操作">3.2HDFS-API相关操作</a></h2>
<blockquote>
<p>github上我仓库的hadoopLearning :<a href="https://github.com/AirlandKK/hadoopLearning">https://github.com/AirlandKK/hadoopLearning</a></p>
</blockquote>
<h1 id="第四章-hdfs的读写流程面试重点"><a class="header" href="#第四章-hdfs的读写流程面试重点">第四章 HDFS的读写流程（面试重点）</a></h1>
<h2 id="41-hdfs写数据流程"><a class="header" href="#41-hdfs写数据流程">4.1 HDFS写数据流程</a></h2>
<h3 id="411-剖析文件写入"><a class="header" href="#411-剖析文件写入">4.1.1 剖析文件写入</a></h3>
<p><img src="bigData/Hadoop/img/HDFSwrite.png" alt="HDFSwrite" /></p>
<p>1、客户端通过Distributed FileSystem模块向namenode请求上传文件到/user/atguigu/ss.avi这个路径文件</p>
<p>2、校验文件是否存在，如果存在就会报目录存在这个错误，如果不存在则相应可以上传文件</p>
<p>3、客户端请求第一个Block（0-128M）上传到那几个DataNode服务器上</p>
<p>4、返回dn1，dn2，dn3节点，表示这三个节点可以存储数据（通过负载量和距离来选择dn）</p>
<p>5、客户端通过调用FSDataOutPutStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信          管道建立完成。</p>
<p>6、dn1、dn2、dn3逐级应答客户端。</p>
<p>7、客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就          会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</p>
<pre><code>  当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。
</code></pre>
<p>8、告诉namenode传输完成</p>
<h3 id="412-网络拓扑-节点距离计算"><a class="header" href="#412-网络拓扑-节点距离计算">4.1.2 网络拓扑-节点距离计算</a></h3>
<p>​	在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。那么这个最近距离怎么计算呢？</p>
<p>​	<strong>节点距离：两个节点到达最近的共同祖先的距离总和。</strong></p>
<p>把n当作服务器，把机架r看作交换机，把d看作机房，把最外面看作互联网，一条线计为1</p>
<p><img src="bigData/Hadoop/img/image-20220108183607093.png" alt="image-20220108183607093" /></p>
<h3 id="413-机架感知副本存储节点选择"><a class="header" href="#413-机架感知副本存储节点选择">4.1.3 机架感知（副本存储节点选择）</a></h3>
<ol>
<li>
<p>机架感知说明 </p>
<ul>
<li>
<blockquote>
<p>官网说明：https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html</p>
<p>For the common case, when the replication factor is three, HDFS’s placement policy is to put <strong>one replica <u>on the local</u> machine</strong> if the writer is on a datanode, otherwise on a random datanode in the same rack as that of the writer, <strong>another replica on a node in a <u>different (remote) rack</u>, and the last on a different node in the <u>same remote</u> rack.</strong> This policy cuts the inter-rack write traffic which generally improves write performance. The chance of rack failure is far less than that of node failure; this policy does not impact data reliability and availability guarantees. However, it does not reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three. With this policy, the replicas of a block do not evenly distribute across the racks. Two replicas are on different nodes of one rack and the remaining replica is on a node of one of the other racks. This policy improves write performance without compromising data reliability or read performance.</p>
</blockquote>
</li>
<li>
<p>第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个，第二个副本在另一个机架的随机一个节点，第三个副本在第二个副本所在机架的随即节点。（<u>保持可靠性兼顾效率</u>）</p>
</li>
</ul>
</li>
<li>
<p>源码说明</p>
<ul>
<li>Crtl+n 查找BlockPlacementPolicyDefault，在该类中chooseTargetInOrder方法。</li>
</ul>
</li>
</ol>
<h2 id="42-hdfs读数据流程"><a class="header" href="#42-hdfs读数据流程">4.2 HDFS读数据流程</a></h2>
<p><img src="bigData/Hadoop/img/20200410190716840.png" alt="img" /></p>
<p>1、客户端通过Distributed FileSystem（客户端对象）向NameNode请求下载文件，创建一个FSDataInputStream（流对象）。（<u>Namenode要判断该请求是否有权限、文件是否存在</u>）</p>
<p>2、NameNode通过查询元数据，找到文件块所在的DataNode地址，返回目标文件的元数据。</p>
<p>3、挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。（<u>判断节点距离、也会判断负载均衡</u>）</p>
<p>4、DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</p>
<p>5、客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</p>
<p>6、最后会关闭资源</p>
<blockquote>
<p>PS ：HDFS的读的方式都是<strong>串行读</strong>！只能串行读！为了保证每一个任务的传输效率，且追求稳定可靠</p>
</blockquote>
<h1 id="第五章-namenode和secondarynamenode"><a class="header" href="#第五章-namenode和secondarynamenode">第五章 NameNode和SecondaryNameNode</a></h1>
<h2 id="51-nn和2nn的关系"><a class="header" href="#51-nn和2nn的关系">5.1 NN和2NN的关系</a></h2>
<pre><code>内存：
	好处：计算快
	坏处：可靠性差
磁盘：
	好处：可靠性高
	坏处：计算慢

内存+磁盘=》效率低
		fsImage 存储数据（如果是随机读写效率 a=10 a+10=&gt;a=20）
		对历史数据的改写效率很低，
		但是可以追加
		Edits追加=&gt;a+10 a-30 a*20 （只记录过程）
		
		
fsImage存储数据+Edits追加=内存  (2NN可以帮助这两个文件定期进行合并)
</code></pre>
<p>工作机制图：</p>
<p><img src="bigData/Hadoop/img/image-20220114120035463.png" alt="image-20220114120035463" /></p>
<p>1 第一阶段： namenode 启动
1）第一次启动 namenode 格式化后， 创建 fsimage 和 edits 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。
2） 客户端对元数据进行增删改的请求。
3） namenode 记录操作日志，更新滚动日志。
4） namenode 在内存中对数据进行增删改查。</p>
<p>2 第二阶段： Secondary NameNode 工作
1） Secondary NameNode 询问 namenode 是否需要 checkpoint。 直接带回 namenode 是否检查结果。
2） Secondary NameNode 请求执行 checkpoint。
3） namenode 滚动正在写的 edits 日志。
4）将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。
5） Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。
6） 生成新的镜像文件 fsimage.chkpoint。
7） 拷贝 fsimage.chkpoint 到 namenode。
8） namenode 将 fsimage.chkpoint 重新命名成 fsimage。</p>
<blockquote>
<p>ps：定时时间默认1小时，Edits数据满了默认100万条</p>
</blockquote>
<p><strong>NN和2NN工作机制详解：</strong></p>
<pre><code>Fsimage：namenode内存中元数据序列化后形成的文件。 
Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。 
namenode启动时，先滚动edits并生成一个空的edits.inprogress，然后加载edits和fsimage到内存中，此时namenode内存就持有最新的元数据信息。client开始对namenode发送元数据的增删改查的请求，这些请求的操作首先会被记录的edits.inprogress中（查询元数据的操作不会被记录在edits中，因为查询操作不会更改元数据信息），如果此时namenode挂掉，重启后会从edits中读取元数据的信息。然后，namenode会在内存中执行元数据的增删改查的操作。 
由于edits中记录的操作会越来越多，edits文件会越来越大，导致namenode在启动加载edits时会很慢，所以需要对edits和fsimage进行合并（所谓合并，就是将edits和fsimage加载到内存中，照着edits中的操作一步步执行，最终形成新的fsimage）。secondarynamenode的作用就是帮助namenode进行edits和fsimage的合并工作。 
secondarynamenode首先会询问namenode是否需要checkpoint（触发checkpoint需要满足两个条件中的任意一个，定时时间到和edits中数据写满了）。直接带回namenode是否检查结果。secondarynamenode执行checkpoint操作，首先会让namenode滚动edits并生成一个空的edits.inprogress，滚动edits的目的是给edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的edits和fsimage会拷贝到secondarynamenode的本地，然后将拷贝的edits和fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给namenode，重命名为fsimage后替换掉原来的fsimage。namenode在启动时就只需要加载之前未合并的edits和fsimage即可，因为合并过的edits中的元数据信息已经被记录在fsimage中。
</code></pre>
<h2 id="52-fsimage镜像文件和edits编辑日志解析"><a class="header" href="#52-fsimage镜像文件和edits编辑日志解析">5.2 Fsimage（镜像文件）和Edits（编辑日志）解析</a></h2>
<ul>
<li>概念</li>
</ul>
<p>NameNode被格式化之后，将在/hadoop-3.1.3/data/tmp/dfs/name/current目录中产生如下文件</p>
<pre><code class="language-shell">[root@ZKK01 current]# ll
total 1044
-rw-r--r-- 1 root root     640 Jan  6 21:18 edits_0000000000000000001-0000000000000000009
-rw-r--r-- 1 root root 1048576 Jan  6 21:18 edits_inprogress_0000000000000000010
-rw-r--r-- 1 root root     399 Jan  6 21:01 fsimage_0000000000000000000
-rw-r--r-- 1 root root      62 Jan  6 21:01 fsimage_0000000000000000000.md5
-rw-r--r-- 1 root root       3 Jan  6 21:18 seen_txid
-rw-r--r-- 1 root root     215 Jan  6 21:01 VERSION
</code></pre>
<ol>
<li>Fsimage文件：HDFS文件系统元数据的一个<strong>永久性的检查点</strong>，其中包含HDFS文件系统的所有目录和文件inode的序列化信息</li>
<li>Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。</li>
<li>seen_txid文件保存的是一个数字，就是最后一个edits_的数字</li>
<li>每次NameNode启动的时候都会将fsimage文件读入内存，加载edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将fsimage和edits文件进行了合并。</li>
</ol>
<ul>
<li>
<p>oiv查看fsimage文件</p>
<ul>
<li>
<p>查看oiv和oev命令 </p>
<pre><code class="language-shell">[root@ZKK01 current]# hdfs
oiv               apply the offline fsimage viewer to an fsimage
oev               apply the offline edits viewer to an edits file
</code></pre>
</li>
<li>
<p>基本语法</p>
<pre><code class="language-sehll">hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径  
</code></pre>
</li>
<li>
<p>举个例子</p>
<pre><code>[root@ZKK01 current]# pwd
/opt/software/hadoop-3.3.1/data/tmp/dfs/name/current
[root@ZKK01 current]# hdfs oiv -p XML -i fsimage_0000000000000000000 -o /opt/software/hadoop-3.3.1/fsimage.xml
2022-01-14 13:39:03,374 INFO offlineImageViewer.FSImageHandler: Loading 2 strings
2022-01-14 13:39:03,546 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
2022-01-14 13:39:03,546 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
2022-01-14 13:39:03,546 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
2022-01-14 13:39:03,546 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
[root@ZKK01 current]# cat /opt/software/hadoop-3.3.1/fsimage.xml 
</code></pre>
</li>
<li>
<p>命令：sz 文件名 可以存到windows</p>
<pre><code class="language-shell">[root@ZKK01 current]# sz /opt/software/hadoop-3.3.1/fsimage.xml 
</code></pre>
</li>
</ul>
</li>
<li>
<p>oev查看edits文件</p>
<ul>
<li>
<p>基本语法</p>
<pre><code class="language-shell">hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径
</code></pre>
</li>
<li>
<p>举个例子</p>
<pre><code class="language-shell">[root@ZKK01 current]# hdfs oev -p XML -i edits_0000000000000000001-0000000000000000009 -o /opt/software/hadoop-3.3.1/edits.xml
</code></pre>
</li>
</ul>
</li>
<li>
<p>思考namenode怎么确定下次合并那些edits？</p>
<ul>
<li>将大于fsimage最后序号的edits文件合并</li>
</ul>
</li>
</ul>
<h2 id="53-checkpoint时间设置"><a class="header" href="#53-checkpoint时间设置">5.3 CheckPoint时间设置</a></h2>
<ol>
<li>
<p>通常情况下，SecondaryNameNode每隔一小时执行一次。 </p>
<p>[hdfs-default.xml]</p>
<pre><code class="language-xml">&lt;property&gt;
  &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;
  &lt;value&gt;3600&lt;/value&gt;
&lt;/property &gt;
</code></pre>
</li>
<li>
<p>一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。</p>
<pre><code class="language-xml">&lt;property&gt;
  &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt;
  &lt;value&gt;1000000&lt;/value&gt;
&lt;description&gt;操作动作次数&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt;
  &lt;value&gt;60&lt;/value&gt;
&lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt;
&lt;/property &gt;
</code></pre>
</li>
</ol>
<h1 id="第六章-datanode"><a class="header" href="#第六章-datanode">第六章 DataNode</a></h1>
<h2 id="61-datanode-工作机制"><a class="header" href="#61-datanode-工作机制">6.1 DataNode 工作机制</a></h2>
<p><img src="bigData/Hadoop/img/20180513223904745" alt="img" /></p>
<ol>
<li>
<p>一个数据块在 datanode 上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</p>
</li>
<li>
<p>DataNode 启动后向 namenode 注册， 通过后，周期性（6小时） 的向 namenode 上报所有的块信息。</p>
<p><img src="bigData/Hadoop/img/image-20220114152438280.png" alt="image-20220114152438280" /></p>
</li>
<li>
<p>心跳是每 3 秒一次，心跳返回结果带有 namenode 给该 datanode 的命令如复制块数据到另一台机器，或删除某个数据块。 如果超过 10 分钟没有收到某个 datanode 的心跳，则认为该节点不可用。</p>
</li>
<li>
<p>集群运行中可以安全加入和退出一些机器。</p>
</li>
</ol>
<h2 id="62-数据完整性"><a class="header" href="#62-数据完整性">6.2 数据完整性</a></h2>
<ol>
<li>
<p>当DataNode读取block的时候，它会计算checksum校验和</p>
</li>
<li>
<p>如果计算后的checksum，与block创建时值不一样，说明block已经损坏。</p>
</li>
<li>
<p>client读取其他DataNode上的block.</p>
</li>
<li>
<p>常见的校验算法crc(32),md5(128),shal(160)</p>
</li>
<li>
<p>datanode在其文件创建后周期验证checksum校验和</p>
</li>
</ol>
<p><img src="bigData/Hadoop/img/20180513224153225" alt="img" /></p>
<h2 id="63-掉线时限参数设置"><a class="header" href="#63-掉线时限参数设置">6.3 掉线时限参数设置</a></h2>
<p>1、DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信;
2、NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长;
3、HDFS默认的超时时长为10分钟+30秒;
4、如果定义超时时间为TimeOut，则超时时长的计算公式为：</p>
<pre><code class="language-shell">TimeOut = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval
</code></pre>
<p>而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。需要注意的是 hdfs-site.xml 配置文件中的heartbeat.recheck.interval 的单位为<strong>毫秒</strong>，dfs.heartbeat.interval 的单位为<strong>秒</strong>。</p>
<pre><code class="language-xml">&lt;property&gt;
 &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt;
 &lt;value&gt;300000&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt;
 &lt;value&gt;3&lt;/value&gt;
&lt;/property&gt;

</code></pre>
<h1 id="总结"><a class="header" href="#总结">总结</a></h1>
<ol>
<li>HDFS文件块大小
<ul>
<li>硬盘读写速度</li>
<li>在企业中 一般128m（中小公司） 256m（大公司）</li>
</ul>
</li>
<li>HDFS的Shell操作（开发重点）</li>
<li>HDFS的读写流程</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><p>[toc]</p>
<h1 id="第一章-mapreduce概述"><a class="header" href="#第一章-mapreduce概述">第一章 MapReduce概述</a></h1>
<h2 id="11-mapreduce-定义"><a class="header" href="#11-mapreduce-定义">1.1 MapReduce 定义</a></h2>
<p>​	MapReduce是一个<strong>分布式运算程序</strong>的编程框架，是用户开发“基于Hadoop的数据分析应用“的核心框架。</p>
<p>​	MapReduce核心功能是将<strong>用户编写的业务逻辑代码</strong>和<strong>自带默认组件</strong>整合成一个完整的<strong>分布式运算程序</strong>，并发运行在一个Hadoop集群上。</p>
<h2 id="12-mapreduce-优缺点"><a class="header" href="#12-mapreduce-优缺点">1.2 MapReduce 优缺点</a></h2>
<h3 id="121-优点"><a class="header" href="#121-优点">1.2.1 优点</a></h3>
<ol>
<li>MapReduce 易于编程 。用户只关心，业务逻辑。实现框架接口。</li>
<li>良好的扩展性：可以动态增加服务器，解决计算资源不够的问题。</li>
<li>高容错性。任何一台机器挂掉，可以将任务转移到其他节点。</li>
<li>适合海量数据计算（TB/PB）几千台服务器共同计算。</li>
</ol>
<h3 id="122-缺点"><a class="header" href="#122-缺点">1.2.2 缺点</a></h3>
<ol>
<li>不擅长实时计算。Mysql</li>
<li>不擅长流式计算。sark flink</li>
<li>不擅长DAG有向无环图计算。spark -rdd 迭代时计算</li>
</ol>
<h1 id="13-mapreduce-核心思想"><a class="header" href="#13-mapreduce-核心思想">1.3 MapReduce 核心思想</a></h1>
<ol>
<li>MapReduce运算程序一般需要分为2个阶段：Map阶段和Reduce阶段</li>
<li>Map阶段的并发MapTask，完全并行运行，互不相干
<ul>
<li>读数据，并按行处理</li>
<li>按空格切分行内单词</li>
<li>KV键值对（单词，1）</li>
<li>将所有的KV键值对中的单词，按照单词首字母，分成2个分区溢写到磁盘</li>
</ul>
</li>
<li>Reduce阶段的并发ReduceTask，完全互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出
<ul>
<li>统计map阶段分组好的单词，输出数据（输出结果到文件）</li>
</ul>
</li>
<li>MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。</li>
</ol>
<p><strong>若干问题细节</strong></p>
<ol>
<li>MapTask如何工作</li>
<li>ReduceTask如何工作</li>
<li>MapTask如何控制分区、排序等</li>
<li>MapTask和ReduceTask之间如何衔接</li>
</ol>
<h1 id="14-mapreduce进程"><a class="header" href="#14-mapreduce进程">1.4 MapReduce进程</a></h1>
<p>一个完整的MapReduce程序在分布式运行时有三类实例进程：</p>
<ol>
<li><strong>MrAppMaster</strong>（任务、job、mr）：负责整个程序的过程调度及状态协调。</li>
<li><strong>MapTask</strong>(查不到，因为叫yarnchild)：负责Map阶段的整个数据处理流程。</li>
<li><strong>ReduceTask</strong>(查不到，因为叫yarnchild)：负责Reduce阶段的整个数据处理流程。</li>
</ol>
<h1 id="15-官方wordcount源码"><a class="header" href="#15-官方wordcount源码">1.5 官方WordCount源码</a></h1>
<p>https://github.com/apache/hadoop</p>
<p>查看wordcount方法</p>
<p>https://github1s.com/apache/hadoop/blob/66b72406bd8bed28ca32c75e07fc2b682500e92b/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordCount.java</p>
<h1 id="16-常用数据序列化类型"><a class="header" href="#16-常用数据序列化类型">1.6 常用数据序列化类型</a></h1>
<p><img src="bigData/Hadoop/img/image-20220316145443719.png" alt="image-20220316145443719" /></p>
<h1 id="17-编程规范"><a class="header" href="#17-编程规范">1.7 编程规范</a></h1>
<p>用户编写的程序分为三个部分：Mapper、Reducer和Driver</p>
<h2 id="1-mapper阶段"><a class="header" href="#1-mapper阶段">1. Mapper阶段</a></h2>
<ol>
<li>
<p>用户自定义的Mapper要继承自己的父类</p>
</li>
<li>
<p>Mapper的输入是KV对的形式（KV的类型可自定义）</p>
</li>
<li>
<p>Mapper中的业务逻辑卸载map()方法汇总</p>
</li>
<li>
<p>Mapper的输出数据是KV对的形式（KV的类型可自定义）</p>
</li>
<li>
<p><strong>map（）方法（MapTask进程）对每一个&lt;k,v&gt;调用一次</strong></p>
<p><img src="bigData/Hadoop/img/image-20220316145748544.png" alt="image-20220316145748544" /></p>
</li>
</ol>
<h2 id="2reducer阶段"><a class="header" href="#2reducer阶段">2.Reducer阶段</a></h2>
<ol>
<li>
<p>用户自定义的Reducer要继承自己的父类</p>
</li>
<li>
<p>Reducer的输入数据类型对应Mapper的输出数据类型，也是KV</p>
</li>
<li>
<p>Reducer的业务逻辑卸载reduce()方法中</p>
</li>
<li>
<p>ReduceTask进程对<strong>每一组相同k</strong>的&lt;k,v&gt;组调用一次reduce()方法</p>
<p><img src="bigData/Hadoop/img/image-20220316150334053.png" alt="image-20220316150334053" /></p>
</li>
</ol>
<h2 id="3-driver阶段"><a class="header" href="#3-driver阶段">3. Driver阶段</a></h2>
<p>相当于YARN集群的客户端，封装了MapReducer相关运行参数的job对象</p>
<p><img src="bigData/Hadoop/img/image-20220316152319674.png" alt="image-20220316152319674" /></p>
<h1 id="代码阶段"><a class="header" href="#代码阶段">代码阶段</a></h1>
<p><img src="bigData/Hadoop/img/image-20220316152039661.png" alt="image-20220316152039661" /></p>
<pre><code class="language-xml">    &lt;build&gt;
        &lt;plugins&gt;
            &lt;!-- 配置当前项目的jdk版本信息 --&gt;
            &lt;plugin&gt;
                &lt;!-- 不包含其他依赖的打包，所以很小 --&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.8.1&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;11&lt;/source&gt;
                    &lt;target&gt;11&lt;/target&gt;
                    &lt;encoding&gt;UTF-8&lt;/encoding&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                 &lt;!-- 包含其他依赖的打包 --&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.3.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;!-- 配置描述符文件 --&gt;
                    &lt;!--                    &lt;descriptor&gt;src/main/assembly/assembly.xml&lt;/descriptor&gt;--&gt;
                    &lt;!-- 也可以使用Maven预配置的描述符--&gt;
                    &lt;descriptorRefs&gt;
                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                    &lt;/descriptorRefs&gt;
                &lt;/configuration&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;make-assembly&lt;/id&gt;
                        &lt;!-- 绑定到package生命周期 --&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;!-- 只运行一次 --&gt;
                            &lt;goal&gt;single&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
</code></pre>
<h1 id="第2章-hadoop的序列化"><a class="header" href="#第2章-hadoop的序列化">第2章 Hadoop的序列化</a></h1>
<h2 id="21-序列化概述"><a class="header" href="#21-序列化概述">2.1 序列化概述</a></h2>
<ol>
<li>结构紧凑：存储空间少</li>
<li>传输快速</li>
<li>互操作性：支持多语言的使用</li>
</ol>
<h2 id="22-自定义bean对象实现序列化接口writable"><a class="header" href="#22-自定义bean对象实现序列化接口writable">2.2 自定义bean对象实现序列化接口（Writable）</a></h2>
<p>具体实现bean对象序列化步骤如下7步。</p>
<p>（1）必须实现Writable接口</p>
<p>（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造</p>
<pre><code class="language-java">public FlowBean() {

   super();

}
</code></pre>
<p>（3）重写序列化方法</p>
<pre><code class="language-java">@Override

public void write(DataOutput out) throws IOException {

   out.writeLong(upFlow);

   out.writeLong(downFlow);

   out.writeLong(sumFlow);

}
</code></pre>
<p>（4）重写反序列化方法</p>
<pre><code class="language-java">@Override

public void readFields(DataInput in) throws IOException {

   upFlow = in.readLong();

   downFlow = in.readLong();

   sumFlow = in.readLong();

}
</code></pre>
<p><strong>（5）注意反序列化的顺序和序列化的顺序完全一致</strong></p>
<p>（6）要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续用。</p>
<p>（7）如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。</p>
<pre><code class="language-java">@Override

public int compareTo(FlowBean o) {

   // 倒序排列，从大到小

   return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;

}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h4 id="1用公司电脑ssh服务器突然连接不上了之前可以"><a class="header" href="#1用公司电脑ssh服务器突然连接不上了之前可以">1.用公司电脑ssh服务器，突然连接不上了（之前可以）</a></h4>
<img src="bigData/Hadoop/img/image-20211229153341947.png" alt="image-20211229153341947" style="zoom:50%;" />
<p>ssh -v root@110.42.160.28 ,报错信息如下</p>
<pre><code class="language-shell">C:\02_projects\myGit\learning&gt;ssh -v root@110.42.160.28                                                                                                  
OpenSSH_for_Windows_8.1p1, LibreSSL 3.0.2
debug1: Connecting to 110.42.160.28 [110.42.160.28] port 22.
debug1: connect to address 110.42.160.28 port 22: Connection refused
ssh: connect to host 110.42.160.28 port 22: Connection refused
</code></pre>
<p>用VNC登录，显示<strong>Failed to start OpenSSH server deamon</strong></p>
<p>接着输入sshd -t 检查：显示Missing privilege separation directory: /var/empty/sshd</p>
<p>解决办法：创建一个目录/var/empty/sshd</p>
<pre><code class="language-shell">mkdir /var/empty
mkdir /var/empty/sshd
sshd -t
#重启sshd
systemctl restart sshd
</code></pre>
<img src="bigData/Hadoop/img/image-20211229162400042.png" alt="image-20211229162400042" style="zoom:50%;" />
<p>后能成功登录</p>
<h4 id="2-伪分布式群起失败"><a class="header" href="#2-伪分布式群起失败">2. 伪分布式群起失败。</a></h4>
<p><img src="bigData/Hadoop/img/image-20211229164421126.png" alt="image-20211229164421126" /></p>
<p>解决方法:</p>
<p>可以把/home/hadoop/.ssh/known_hosts文件删了，然后重新生成配对密钥即可</p>
<pre><code class="language-shell">sudo apt-get openssh-server
</code></pre>
<pre><code class="language-shell">ssh-keygen -t rsa -P &quot;&quot;
</code></pre>
<pre><code class="language-shell">cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys
</code></pre>
<p><a href="https://blog.csdn.net/weixin_30619101/article/details/96996016?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&amp;utm_relevant_index=1">ssh连接所生成的known_hosts出现的问题</a></p>
<h4 id="3-解决云服务器重启后hostname还原的问题"><a class="header" href="#3-解决云服务器重启后hostname还原的问题">3. 解决云服务器重启后，hostname还原的问题</a></h4>
<pre><code class="language-text">查看主机名：hostname
修改主机名：
方法1：sudo hostname xxx 
             但是这是临时的，重启后失效
方法2 ：修改hostname文件，永久修改
              sudo vi /etc/hostname
              重启系统后才会生效
但是，在云服务器上，用方法2设置后，重启后还是会将hostname还原为之前的
需要在在 /etc/cloud/cloud.cfg中将cloud_init_modules中的下面两行删除
-set_hostname
- [update_hostname,once-per-instance]
</code></pre>
<p>linux centos yum报错 To address this issue please refer to the below wiki article 解决方法</p>
<img src="bigData/Hadoop/img/image-20220101212939753.png" alt="image-20220101212939753" style="zoom:33%;" />
<p>报错原因：国外<a href="https://so.csdn.net/so/search?q=yum">yum</a>镜像源 国内下载不了 修改为国内阿里yum镜像源</p>
<p>解决方法：</p>
<pre><code class="language-bash">cd /etc/yum.repos.d/
mkdir repo_bak
mv *.repo repo_bak/
wget http://mirrors.aliyun.com/repo/Centos-7.repo
yum clean all
yum makecache
</code></pre>
<p>详细参考：《<a href="https://so.csdn.net/so/search?q=centos7">centos7</a> 配置国内yum源和epel源》https://blog.csdn.net/whatday/article/details/106107168</p>
<p>解析不不了配置文件中的$releasever</p>
<p><img src="bigData/Hadoop/img/image-20220101213224717.png" alt="image-20220101213224717" /></p>
<pre><code class="language-shell">vim Centos-7.repo
vim CentOS-Base.repo
#用vim的查找替换命令 将$releasever都替换为7（因为我的是centos7）
%s/\$releasever/7/g
</code></pre>
<h4 id="4vim-字符串替换"><a class="header" href="#4vim-字符串替换">4.Vim 字符串替换</a></h4>
<p>查找和替换是任意一款文本编辑器的一组常见和必备功能。下面就来讲解 Vim 中的字符串替换功能。</p>
<p>Vim 使用以下命令结构实现替换功能。</p>
<pre><code>:&lt;range&gt; s/&lt;search_string&gt;/&lt;replace_string&gt;/&lt;modifier&gt;
</code></pre>
<ul>
<li>range - 定义执行“查找和替换”函数的范围，有两个不同的值
<ul>
<li>％ - 对整个文件执行</li>
<li>&lt; start _line &gt; &lt; end_line &gt; - 在一组特定的行上面执行操作</li>
</ul>
</li>
<li>search_string - 需要替换的字符串</li>
<li>replace_string - 替换旧字符串的新字符串</li>
<li>modifier - 确定替换行为，有几个不同的值
<ul>
<li>g - 全局替换</li>
<li>gc - 在每次更换之前要求确认</li>
<li>gn - 忽略替换功能并突出显示查找结果。</li>
</ul>
</li>
</ul>
<h4 id="5namenode无法启动报错原因"><a class="header" href="#5namenode无法启动报错原因">5.NameNode无法启动，报错原因：</a></h4>
<p>1、 java.net.BindException: Port in use: master:9001</p>
<p>2、Caused by: java.net.BindException: Cannot assign requested address</p>
<img src="bigData/Hadoop/img/image-20220101223611130.png" alt="image-20220101223611130" style="zoom:33%;" />
<p>端口被占用是直接原因，但起因是不能分配所需的地址，跟地址有关的就联想到 /etc/hosts文件</p>
<p>云服务器的IP要换成内网的IP，内网可以比作一个局域网。 </p>
<h4 id="6hadoop集群部署上后在服务器中运行hadoop自带的jar包中的实例报错"><a class="header" href="#6hadoop集群部署上后在服务器中运行hadoop自带的jar包中的实例报错">6.hadoop集群部署上后，在服务器中运行hadoop自带的jar包中的实例报错</a></h4>
<p><img src="bigData/Hadoop/img/image-20220103164220531.png" alt="image-20220103164220531" /></p>
<p>解决方法：按错误提示，在mapred-site.xml配置文件中添加hadoop根目录</p>
<p>1.先运行hadoop classpath得到classpath</p>
<p>将得到的classpath全部复制到mapred-site.xml中，配置</p>
<pre><code class="language-xml">&lt;property&gt; 
    &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;    &lt;value&gt;/home/hadoop/app/hadoop/etc/hadoop:/home/hadoop/app/hadoop/share/hadoop/common/lib/*:/home/hadoop/app/hadoop/share/hadoop/common/*:/home/hadoop/app/hadoop/share/hadoop/hdfs:/home/hadoop/app/hadoop/share/hadoop/hdfs/lib/*:/home/hadoop/app/hadoop/share/hadoop/hdfs/*:/home/hadoop/app/hadoop/share/hadoop/mapreduce/*:/home/hadoop/app/hadoop/share/hadoop/yarn:/home/hadoop/app/hadoop/share/hadoop/yarn/lib/*:/home/hadoop/app/hadoop/share/hadoop/yarn/*
&lt;/value&gt;
&lt;/property&gt;

</code></pre>
<p>配置结束关闭mapred-site.xml</p>
<p>重新启动集群，再在share文件中运行</p>
<h4 id="7-warning-remote-host-identification-has-changed"><a class="header" href="#7-warning-remote-host-identification-has-changed">7. WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!</a></h4>
<p>报错如下</p>
<pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
ZKK01: @    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
ZKK01: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
ZKK01: IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
ZKK01: Someone could be eavesdropping on you right now (man-in-the-middle attack)!
ZKK01: It is also possible that a host key has just been changed.
ZKK01: The fingerprint for the ECDSA key sent by the remote host is
...
</code></pre>
<p><strong>原因：</strong></p>
<p>因为服务器的ip发生变更了
第一次SSH连接时，会生成一个认证，储存在客户端（也就是用SSH连线其他电脑的那个，自己操作的那个）中的known_hosts，但是如果服务器验证过了，认证资讯当然也会更改，服务器端与客户端不同时，就会跳出错误啦。</p>
<p><strong>解决办法：</strong></p>
<pre><code class="language-shell">输入命令：ssh-keygen -R +输入服务器的IP
</code></pre>
<h4 id="8permission-denied-publickeygssapi-keyexgssapi-with-micpassword问题解决"><a class="header" href="#8permission-denied-publickeygssapi-keyexgssapi-with-micpassword问题解决">8.Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password)问题解决</a></h4>
<p>经过排查发现是没有设置免密登录，解决方案如下：</p>
<pre><code class="language-shell">ssh-keygen -t rsa
cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys
</code></pre>
<h4 id="9hdfs服务器使用命令可以上传文件但客户端上传失败问题file--could-only-be-written-to-0-of-the-1-minreplication-nodes"><a class="header" href="#9hdfs服务器使用命令可以上传文件但客户端上传失败问题file--could-only-be-written-to-0-of-the-1-minreplication-nodes">9.HDFS服务器使用命令可以上传文件,但客户端上传失败问题FILE ~ COULD ONLY BE WRITTEN TO 0 OF THE 1 MINREPLICATION NODES.</a></h4>
<p><img src="bigData/Hadoop/img/image-20220106195248136.png" alt="image-20220106195248136" /></p>
<p>然后接下来我就在我的服务器里上传了一个文件试试，发现上传成功，没什么问题</p>
<p>为什么服务器里可以成功上传文件，而客户端却不行呢(客户端能上传文件，但是只有一个空壳，内部并没有数据)？但是客户端又可以成功创建文件夹目录呀！？</p>
<p><strong>思考：我们的文件目录，文件名是存放在哪里？而我们的具体数据又是写在哪里？</strong></p>
<blockquote>
<p><em>NameNode节点存放的是文件目录，也就是文件夹、文件名称，本地可以通过公网访问 NameNode，所以可以进行文件夹的创建，当上传文件需要写入数据到DataNode时，NameNode 和DataNode 是通过局域网进行通信，NameNode返回地址为 DataNode 的私有 IP，本地无法访问</em></p>
</blockquote>
<p><strong>解决方案</strong></p>
<p>返回的IP地址无法返回公网IP，只能<code>返回主机名</code>，通过主机名与公网地址的映射便可以访问到DataNode节点，问题将解决。</p>
<p>由于<code>代码的设置的优先级为最高</code>，所以直接进行代码的设置。</p>
<pre><code class="language-java">Configuration conf = new Configuration();
conf.set(&quot;dfs.client.use.datanode.hostname&quot;, &quot;true&quot;);//添加此配置信息即可
FileSystem fs = FileSystem.get(new URI(&quot;hdfs://host:9000&quot;), conf, &quot;root&quot;);
</code></pre>
<blockquote>
<p>注意：</p>
<ol>
<li>
<p>主机名我们可以通过下面的命令查看：</p>
<pre><code class="language-bash">   hadoop dfsadmin -report
</code></pre>
</li>
<li>
<p>本地还需要配置一下DNS域名映射，在<code>C:\Windows\System32\drivers\etc的hosts文件中</code>配置</p>
</li>
<li>
<p>腾讯云等服务器还需要配置一下我们的安全组、防火墙9866</p>
</li>
<li>
<p>接下来就是我们的防火墙开放一下端口就行了</p>
</li>
</ol>
</blockquote>
<p>附：当然我们也可以直接在配置文件(项目下的配置文件和服务器配置文件都行)中进行一个简单配置：</p>
<pre><code class="language-xml">&lt;property&gt;
    &lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
    &lt;description&gt;only cofig in clients&lt;/description&gt;
&lt;/property&gt;

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="常用端口号及配置文件"><a class="header" href="#常用端口号及配置文件">常用端口号及配置文件</a></h1>
<ul>
<li>
<p>hadoop3.x</p>
<ul>
<li>HDFS NameNode 内部常用端口：8020/9000/9820</li>
<li>HDFS NameNode 对用户的查询端口：9870</li>
<li>Yarn查看任务运行情况的：8088</li>
<li>历史服务器：19888</li>
</ul>
</li>
<li>
<p>hadoop2.x</p>
<ul>
<li>HDFS NameNode 内部常用端口：8020/9000</li>
<li>HDFS NameNode 对用户的查询端口：50070</li>
<li>Yarn查看任务运行情况的：8088</li>
<li>历史服务器：19888</li>
</ul>
</li>
<li>
<p>常用配置文件</p>
<ul>
<li>3.x
<ul>
<li>core-site.xml</li>
<li>hdfs-site.xml</li>
<li>yarn-site.xml</li>
<li>mapred-site.xml</li>
<li>wokers</li>
</ul>
</li>
<li>2.x
<ul>
<li>core-site.xml</li>
<li>hdfs-site.xml</li>
<li>yarn-site.xml</li>
<li>mapred-site.xml</li>
<li>slaves </li>
</ul>
</li>
</ul>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </body>
</html>
